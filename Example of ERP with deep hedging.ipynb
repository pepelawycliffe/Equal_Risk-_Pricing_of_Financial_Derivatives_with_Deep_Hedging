{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equal Risk Pricing of Financial Derivatives with Deep Hedging \n",
    "This notebook presents an example of implementation of the equal risk pricing framework as presented in Carbonneau and Godin (2020). \n",
    "- The equal risk option price $C_{0}^{\\star}$ and the measure of market incompleteness $\\epsilon^{\\star}$ are computed for a European-type put option of maturity 60 days under the Black-Scholes model. \n",
    "- The deep hedging algorithm of Buehler et al. (2019) is applied twice to train two distinct neural networks which are used to approximate the optimal hedging strategy respectively for the long and the short position in the derivative.\n",
    "- The convex risk measure used to measure the risk of each counterparty is the Conditional Value-at-Risk with alpha = 0.95. \n",
    "- For a complete description of the algorithm, the reader is referred to section 3 of Carbonneau and Godin (2020). \n",
    "\n",
    "Important note: some parts of the code are inspired by the following implementation of the deep hedging algorithm: \n",
    "    - https://github.com/mgroncki/DataScienceNotebooks/blob/master/DeepHedging/DeepHedging_Part1.ipynb  \n",
    "    - https://nbviewer.jupyter.org/urls/people.math.ethz.ch/~jteichma/lecture_ml_web/lecture_3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Construction of the train and test sets\n",
    "- A) Simulation of 500K paths under the Black-Scholes model with the parameters [sigma, mu] = [0.1952, 0.0892] on a yearly scale. \n",
    "    - The feature vector for this model is $X_{n} = [\\log(S_{n}/K), V_{n}, T-t_{n}]$ where $V_{n}$ is the hedging portfolio value. The log-moneyness transformation of the stock price was found to improve the learning of neural networks.  \n",
    "- B) Split into train and test sets with 400K and 100K paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "[sigma, mu]  = [0.1952, 0.0892] # Yearly parameters\n",
    "S_0          = 100     # Initial stock price\n",
    "strike       = 100     # Strike price\n",
    "T            = 60/260  # Time-to-maturity of the vanilla put option\n",
    "n_sims       = 500000  # Total number of paths to simulate\n",
    "n_timesteps  = 60      # Daily hedging\n",
    "r            = 0.02    # Annualized continuous risk-free rate\n",
    "\n",
    "# Simulation of BSM dataset \n",
    "seed           = 10\n",
    "h              = T / n_timesteps  # step-size\n",
    "Price_mat      = np.zeros((n_timesteps+1, n_sims))  # matrix of simulated stock prices \n",
    "Price_mat[0,:] = S_0\n",
    "for i in range(n_sims):\n",
    "    rand_stdnorm     = np.random.randn(n_timesteps)\n",
    "    Price_mat[1:,i]  = S_0 * np.cumprod(np.exp((mu-sigma**2/2)*h+sigma*np.sqrt(h)*rand_stdnorm))\n",
    "\n",
    "# Apply a transformation to stock prices\n",
    "prepro_stock = \"Log-moneyness\"  # {Log, Log-moneyness, Nothing}\n",
    "if(prepro_stock == \"Log\"):\n",
    "    Price_mat = np.log(Price_mat)\n",
    "elif(prepro_stock == \"Log-moneyness\"):\n",
    "    Price_mat = np.log(Price_mat/strike)\n",
    "\n",
    "# Construct the train and test sets\n",
    "# - The feature vector for now is [S_n, T-t_n]; the portfolio value V_{n} will be added further into the code at each time-step\n",
    "train_input     = np.zeros((n_timesteps+1, 400000,2))\n",
    "test_input      = np.zeros((n_timesteps+1, 100000,2))\n",
    "time_to_mat     = np.zeros(n_timesteps+1)\n",
    "time_to_mat[1:] = T / (n_timesteps)      # [0,h,h,h,..,h]                     \n",
    "time_to_mat     = np.cumsum(time_to_mat) # [0,h,2h,...,Nh]\n",
    "time_to_mat     = time_to_mat[::-1]      # [Nh, (N-1)h,...,h,0]\n",
    "\n",
    "train_input[:,:,0] = Price_mat[:,0:400000]  \n",
    "train_input[:,:,1] = np.reshape(np.repeat(time_to_mat, train_input.shape[1], axis=0), (n_timesteps+1, train_input.shape[1]))\n",
    "test_input[:,:,0]  = Price_mat[:,400000:]\n",
    "test_input[:,:,1]  = np.reshape(np.repeat(time_to_mat, test_input.shape[1], axis=0), (n_timesteps+1, test_input.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Hyperparameters of neural networks as in Section 4.1.2 of the paper and the risk aversion parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size    = 1000   # batch size\n",
    "epochs        = 100    # number of epochs\n",
    "hidden_layers = 2      # number of hidden layers (output layer not included, for a total of three layers)\n",
    "nbs_units     = 56     # neurons per layer\n",
    "lr            = 0.0005 # learning rate of the Adam optimizer\n",
    "alpha         = 0.95   # confidence level of the CVaR risk measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Tensor of discount factors\n",
    "- disc_mat: (n_timesteps+1 x n_sims x 1) tensor of cumulative discount rates (discount from t to 0), i.e. $\\exp(-rh*n)$ \n",
    "    - Will be split into disc_mat_train and disc_mat_test for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc_mat              = np.zeros((n_timesteps+1, 500000, 1))\n",
    "disc_train            = np.zeros((n_timesteps+1, 400000, 1))\n",
    "disc_test             = np.zeros((n_timesteps+1, 100000, 1))\n",
    "discount_vect         = np.ones(disc_mat.shape[0])   \n",
    "h                     = T / n_timesteps\n",
    "discount_vect[1:]     = np.exp(-r*h)              # [1,exp(-rh), exp(-rh),....,exp(-rh)]\n",
    "discount_vect         = np.cumprod(discount_vect) # [1,exp(-rh), exp(-r2h),....,exp(-rNh)]\n",
    "disc_mat              = np.reshape(np.repeat(discount_vect, 500000), (n_timesteps+1, 500000,1))\n",
    "disc_train            = disc_mat[:,0:400000,:]\n",
    "disc_batch            = disc_mat[:,0:batch_size,:]  # for convenience when only batch-size is needed\n",
    "disc_test             = disc_mat[:,400000:,:]\n",
    "disc_mat              = []  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Deep hedging class\n",
    "- This class implements the deep hedging algorithm of Buehler et al. (2019) with a feedforward neural network as described in section 3.3 of Carbonneau and Godin (2020). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAgent(object):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    nbs_point_traj: if [S_0,...,S_N] ---> nbs_point_traj = N+1\n",
    "    batch_size    : size of mini-batch\n",
    "    nbs_input     : number of features (always 2 in this example)\n",
    "    position_type : {Long, Short}\n",
    "    hidden_layers : number of hidden layers (doesn't include the output layer) \n",
    "    nbs_units     : number of neurons per layer\n",
    "    lr            : learning rate hyperparameter of the Adam optimizer\n",
    "    prepro_stock  : {Log, Log-moneyness, Nothing} - what transformation was used for stock prices\n",
    "    name          : name to load the trained neural network once trained\n",
    "    \"\"\"\n",
    "    def __init__(self, nbs_point_traj, batch_size, nbs_input, position_type, hidden_layers, nbs_units, lr, prepro_stock, name):\n",
    "    \n",
    "        tf.reset_default_graph() \n",
    "        self.nbs_point_traj = nbs_point_traj\n",
    "        self.batch_size     = batch_size\n",
    "        self.nbs_input      = nbs_input\n",
    "        self.position_type  = position_type\n",
    "        self.hidden_layers  = hidden_layers\n",
    "        self.nbs_units      = nbs_units\n",
    "        self.lr             = lr        \n",
    "        self.prepro_stock   = prepro_stock\n",
    "            \n",
    "        # 1) Placeholder\n",
    "        self.input        = tf.placeholder(tf.float32, [nbs_point_traj, batch_size, nbs_input])  # Stock prices are normalized here\n",
    "        self.strike       = tf.placeholder(tf.float32, batch_size)                    \n",
    "        self.alpha        = tf.placeholder(tf.float32)   # Risk aversion parameter of CVaR risk measure\n",
    "        self.disc_tensor  = tf.placeholder(tf.float32, [nbs_point_traj, batch_size, 1]) # To discount from 't' to 0.\n",
    "        self.deltas       = tf.zeros(shape = [nbs_point_traj-1, batch_size, 1], dtype=tf.float32)\n",
    "        \n",
    "        # 2) Discount prices computation:\n",
    "        self.unorm_price = self.inverse_processing(self.input[:,:,0], prepro_stock)   # Inverse normalization\n",
    "        discount_price   = tf.multiply(self.unorm_price, self.disc_tensor[:,:,0])     # exp(-rh*n)*S_{n}  \n",
    "        inc_disc_ret     = discount_price[1:,:] - discount_price[0:-1,:]              # Increment of discount returns\n",
    "        \n",
    "        # 3) Payoff of each path for an ATM put option\n",
    "        self.payoff = tf.maximum(self.strike - self.unorm_price[-1,:],0)\n",
    "\n",
    "        # 4) Compute the deltas of the strategy with the deep hedging algorithm\n",
    "        # 4.1) Two hidden layers with relu activation function\n",
    "        layer_1 = tf.layers.Dense(self.nbs_units, tf.nn.relu)\n",
    "        layer_2 = tf.layers.Dense(self.nbs_units, tf.nn.relu)\n",
    "        \n",
    "        # 4.2) Output layer of dimension one (outputs the position in the underlying)\n",
    "        layer_out = tf.layers.Dense(1, None)\n",
    "           \n",
    "        # 4.3) Compute hedging strategies for all time-steps        \n",
    "        V_t = tf.zeros(self.batch_size)  # initial portfolio value is always zero\n",
    "        for t in range(self.nbs_point_traj-1):                \n",
    "\n",
    "            # input of the FFNN at time 't': [S_t, T-t, V_t]\n",
    "            input_t = tf.concat([self.input[t,:,:], tf.expand_dims(V_t, axis = 1)], axis=1)   \n",
    "\n",
    "            # forward prop at time 't'\n",
    "            layer = layer_1(input_t)\n",
    "            layer = layer_2(layer)\n",
    "            layer = layer_out(layer)\n",
    "                    \n",
    "            # Compile the trading strategies\n",
    "            if (t==0):\n",
    "                # At t = 0, need to expand the dimension to have [nbs_point_traj, batch_size, 1]\n",
    "                self.deltas = tf.expand_dims(layer,axis=0)      # [1, batch_size, 1]\n",
    "\n",
    "            else:\n",
    "                self.deltas = tf.concat([self.deltas, tf.expand_dims(layer, axis = 0)], axis = 0)\n",
    "                \n",
    "            # Compute the portoflio value for the next period   \n",
    "            factor  = tf.div(self.disc_tensor[t,:,0], self.disc_tensor[t+1,:,0]) # equals exp(rh)\n",
    "            V_t_pre = V_t\n",
    "            # Update rule from Godin (2016), see eq. (1)\n",
    "            V_t     = V_t_pre*factor + self.deltas[t,:,0]*(self.unorm_price[t+1,:] - self.unorm_price[t,:]*factor)\n",
    " \n",
    "        \n",
    "        # 5) Compute the discounted gain at maturity for each paths \n",
    "        cumulative_factor  = tf.reciprocal(self.disc_tensor[-1,:,0])                 # Project from 0 to T\n",
    "        self.disc_gain     = tf.reduce_sum(inc_disc_ret*self.deltas[:,:,0], axis=0)  # G_{N}\n",
    "        \n",
    "        # 6) Compute hedging errors for each paths as if initial portfolio value is zero\n",
    "        if(self.position_type == 'Short'):\n",
    "            self.hedging_err = self.payoff - cumulative_factor*self.disc_gain\n",
    "        elif(self.position_type == 'Long'):\n",
    "            self.hedging_err = - self.payoff - cumulative_factor*self.disc_gain\n",
    "        \n",
    "        # 7) Compute the CVaR_{alpha} on the batch of hedging error\n",
    "        # - This is the empirical cost functions estimated with a mini-batch\n",
    "        # - Equivalent to estimator of CVaR_{alpha} presented in the paper of Carbonneau and Godin (2020) \n",
    "        #   since alpha*N_{batch} is an integer in this example\n",
    "        self.loss = tf.reduce_mean(tf.contrib.framework.sort(self.hedging_err)[tf.cast(self.alpha*self.batch_size,tf.int32):])\n",
    "        \n",
    "        # 8) SGD step with the adam optimizer\n",
    "        optimizer  = tf.train.AdamOptimizer(learning_rate = lr)  \n",
    "        self.train = optimizer.minimize(self.loss) \n",
    "        \n",
    "        # 9) Save the model\n",
    "        self.saver      = tf.train.Saver()\n",
    "        self.model_name = name   # name of the neural network to save\n",
    "          \n",
    "    # Function to compute the CVaR_{alpha} outside the optimization, i.e. at the end of each epoch in this case\n",
    "    def loss_out_optim(self, hedging_err, alpha):\n",
    "        loss = np.mean(np.sort(hedging_err)[int(alpha*hedging_err.shape[0]):])\n",
    "        return loss\n",
    "    \n",
    "    # Given a type of preprocessing, reverse the processing of the stock price\n",
    "    def inverse_processing(self, paths, prepro_stock):\n",
    "        if (prepro_stock ==\"Log-moneyness\"):\n",
    "            paths = tf.multiply(self.strike, tf.exp(paths))\n",
    "        elif (prepro_stock == \"Log\"):\n",
    "            paths = tf.exp(paths)\n",
    "        return paths\n",
    "    \n",
    "    # ---------------------------------------------------------------------------------------# \n",
    "    # Function to call the deep hedging algorithm batch-wise\n",
    "    # Disclore: function adapted from https://github.com/mgroncki/DataScienceNotebooks/blob/master/DeepHedging/DeepHedging_Part1.ipynb \n",
    "    \"\"\"\n",
    "    Input:\n",
    "     - paths       : tensor of features of dimension [n_timesteps+1, n_sims, nbs_features]\n",
    "     - strikes     : vector of strike prices of dimension [N_batch]\n",
    "     - disc_batch  : tensor of discount factors of dimension [n_timesteps+1, N_batch, 1]\n",
    "     - alpha       : level of risk aversion of the hedger\n",
    "     - epochs      : total number of epochs to run\n",
    "    \"\"\"\n",
    "    def train_deephedging(self, paths, strikes, disc_batch, alpha, sess, epochs):\n",
    "        sample_size       = paths.shape[1]               # total number of paths in the train set\n",
    "        batch_size        = self.batch_size    \n",
    "        idx               = np.arange(sample_size)       # [0,1,...,sample_size-1]\n",
    "        start             = dt.datetime.now()            # Time-to-train\n",
    "        self.loss_epochs  = 9999999*np.ones(epochs)      # Store the loss at the end of each epoch for the train              \n",
    "        epoch             = 0                       \n",
    "        \n",
    "        # Loop for each epoch until the maximum number of epochs\n",
    "        while (epoch < epochs):\n",
    "            hedging_err_train = []  # Store hedging errors obtained for one complete epoch \n",
    "            np.random.shuffle(idx)  # Randomize the dataset (not useful in this case since dataset is simulated iid)\n",
    "            \n",
    "            # loop over each batch size\n",
    "            for i in range(int(sample_size/batch_size)):\n",
    "                \n",
    "                # Indexes of paths used for the mini-batch\n",
    "                indices = idx[i*batch_size : (i+1)*batch_size]\n",
    "                                \n",
    "                # SGD step \n",
    "                _, hedging_err = sess.run([self.train, self.hedging_err], \n",
    "                                               {self.input        : paths[:,indices,:],\n",
    "                                                self.strike       : strikes,\n",
    "                                                self.alpha        : alpha,\n",
    "                                                self.disc_tensor  : disc_batch})\n",
    "            \n",
    "                hedging_err_train.append(hedging_err)\n",
    "            \n",
    "            # Store the loss on the train set after each epoch for learning curve\n",
    "            self.loss_epochs[epoch] = self.loss_out_optim(np.concatenate(hedging_err_train), alpha)\n",
    "            \n",
    "            # Print the CVaR value at the end of each epoch\n",
    "            print('Time elapsed:', dt.datetime.now()-start)\n",
    "            print('Epoch %d, CVaR - Train: %.3f' % (epoch+1, self.loss_epochs[epoch]))\n",
    "                \n",
    "            epoch+=1  # increment the epoch\n",
    "                \n",
    "        # End of training\n",
    "        print(\"---Finished training results---\")\n",
    "        print('Time elapsed:', dt.datetime.now()-start)    \n",
    "        self.saver.save(sess, r\"/Users/alexa/ERP paper example/Models/%s/models.ckpt\" % self.model_name)\n",
    "  \n",
    "        # Return the learning curve \n",
    "        return self.loss_epochs\n",
    "\n",
    "    # Function which will call the deep hedging optimization batchwise\n",
    "    def training(self, paths, strikes, disc_batch, riskaversion, sess, epochs):\n",
    "        sess.run(tf.global_variables_initializer()) \n",
    "        loss_train_epoch = self.train_deephedging(paths, strikes, disc_batch, riskaversion, sess, epochs)\n",
    "        return loss_train_epoch\n",
    "\n",
    "    # ---------------------------------------------------------------------- #\n",
    "    # Function to compute the hedging strategies of a trained neural network\n",
    "    # - Doesn't train the neural network, only outputs the hedging strategies\n",
    "    def predict(self, paths, strikes, disc_paths, alpha, sess):\n",
    "        sample_size = paths.shape[1]\n",
    "        batch_size  = self.batch_size    \n",
    "        idx         = np.arange(sample_size)  # [0,1,...,sample_size-1]\n",
    "        start       = dt.datetime.now()     # compute time\n",
    "        strategy_pred = [] # hedging strategies\n",
    "            \n",
    "        # loop over sample size to do one complete epoch\n",
    "        for i in range(int(sample_size/batch_size)):\n",
    "                \n",
    "            # mini-batch of paths (even if not training to not get memory issue)\n",
    "            indices = idx[i*batch_size : (i+1)*batch_size]  \n",
    "            _, strategy = sess.run([self.hedging_err, self.deltas], \n",
    "                                    {self.input        : paths[:,indices,:],\n",
    "                                     self.strike       : strikes,\n",
    "                                     self.alpha        : alpha, \n",
    "                                     self.disc_tensor  : disc_paths})\n",
    "            \n",
    "            # Append the batch of hedging strategies\n",
    "            strategy_pred.append(strategy)\n",
    "        return np.concatenate(strategy_pred,axis=1) \n",
    "    \n",
    "    def restore(self, sess, checkpoint):\n",
    "        self.saver.restore(sess, checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Function to compute the measured risk exposure given a sample of paths and hedging decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------#\n",
    "#---------------      Functions to evaluate the measured risk exposure       ---------------------#\n",
    "#-------------------------------------------------------------------------------------------------#\n",
    "\"\"\" \n",
    "Input:\n",
    "- deltas       : (time step x nbs of paths) \n",
    "- paths        : (time step x nbs of paths)  \n",
    "- disc_paths   : (time step x nbs of paths x 1)\n",
    "- alpha        : for the CVaR computation\n",
    "- position_type: {Long, Short}\n",
    "- prepro_stock : {Log, Log-moneyness, Nothing} - what transformation was used for stock prices\n",
    "\"\"\"\n",
    "#-------------------------------------------------------------------------------------------------#\n",
    "def measured_risk_exposures(deltas, paths, disc_paths, strike, position_type, prepro_stock):\n",
    "    \n",
    "    # De-normalize the stock price process\n",
    "    if(prepro_stock == \"Log\"):\n",
    "        paths = np.exp(paths)\n",
    "    elif(prepro_stock == \"Log-moneyness\"):\n",
    "        paths = strike*np.exp(paths)\n",
    "        \n",
    "    # Discounted prices and returns for each paths at each timesteps\n",
    "    discount_price    = np.multiply(paths, disc_paths[:,:,0])          # exp(-rh*n)*S_{n} \n",
    "    inc_disc_ret      = discount_price[1:,:] - discount_price[0:-1,:]  # Increment of discount returns\n",
    "    \n",
    "    # Option payoff of vanilla ATM put\n",
    "    option_payoff     = np.maximum(strike - paths[-1,:],0) \n",
    "    \n",
    "    # Discounted gain process\n",
    "    disc_gain         = np.sum(deltas*inc_disc_ret, axis = 0)  # G_{N}\n",
    "    cumulative_factor = np.reciprocal(disc_paths[-1,:,0])      # B_{N}\n",
    "     \n",
    "    if(position_type == 'Short'):\n",
    "        hedging_err = option_payoff - cumulative_factor*disc_gain\n",
    "    elif(position_type == 'Long'):\n",
    "        hedging_err = - option_payoff - cumulative_factor*disc_gain\n",
    "        \n",
    "    # Empirical CVaR at level alpha\n",
    "    loss = np.mean(np.sort(hedging_err)[int(alpha*hedging_err.shape[0]):]) \n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Training of the short neural network and computation of the short measured risk exposure $$\\epsilon^{(S)}(0) = \\underset{\\delta \\in \\Pi}{\\min} \\, \\rho \\left(\\Phi(S_{N},Z_{N}) - B_{N}G_{N}^{\\delta}\\right).$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: learning rate=0.0005, layers=2, neurons=56\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'reset_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-2982e786ba2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Compile the neural network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m FFNN_short = DeepAgent(train_input.shape[0], batch_size, train_input.shape[2], position_type, hidden_layers, \n\u001b[0m\u001b[0;32m      7\u001b[0m                         nbs_units, lr, prepro_stock, model_name)\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-1edb3eb44a9d>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, nbs_point_traj, batch_size, nbs_input, position_type, hidden_layers, nbs_units, lr, prepro_stock, name)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbs_point_traj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbs_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposition_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbs_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprepro_stock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnbs_point_traj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnbs_point_traj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'reset_default_graph'"
     ]
    }
   ],
   "source": [
    "position_type = 'Short'\n",
    "model_name    = \"Short_FFNN\"\n",
    "print(\"Hyperparameters: learning rate=%.4f, layers=%d, neurons=%d\" %(lr, hidden_layers, nbs_units))\n",
    "\n",
    "# Compile the neural network\n",
    "FFNN_short = DeepAgent(train_input.shape[0], batch_size, train_input.shape[2], position_type, hidden_layers, \n",
    "                        nbs_units, lr, prepro_stock, model_name)\n",
    "    \n",
    "# Start training \n",
    "start = dt.datetime.now()\n",
    "print('---Training start---')\n",
    "with tf.Session() as sess:\n",
    "    loss_train_epoch = FFNN_short.training(train_input, np.ones(batch_size)*strike, disc_batch, alpha, sess, epochs)\n",
    "   \n",
    "    print('---Training end---')\n",
    "\n",
    "# Plot the learning curve on the train set, i.e. the CVaR on the train set at the end of each epoch\n",
    "lin_nb_epoch = np.linspace(1, loss_train_epoch.shape[0], loss_train_epoch.shape[0])\n",
    "fig = plt.figure(figsize=(5, 5), dpi=100)\n",
    "plt.plot(lin_nb_epoch[1:], loss_train_epoch[1:])\n",
    "plt.title('CVaR alpha = 0.95 of short position for ATM put option')\n",
    "plt.show()\n",
    "\n",
    "# Compute measured risk exposure on train and valid set\n",
    "model_predict = DeepAgent(test_input.shape[0], batch_size, test_input.shape[2], position_type, hidden_layers, \n",
    "                          nbs_units, lr, prepro_stock, model_name)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Load the saved model\n",
    "    model_predict.restore(sess, r\"/Users/alexa/ERP paper example/Models/%s/models.ckpt\" % model_name)\n",
    "    \n",
    "    # Compute hedging strategies on the training set\n",
    "    deltas = model_predict.predict(train_input, np.ones(batch_size)*strike, disc_batch, alpha, sess)\n",
    "    \n",
    "    # Compute CVaR 95% on the train set\n",
    "    CVaR_95_short_train =  measured_risk_exposures(deltas[:,:,0], train_input[:,:,0], disc_train, strike, position_type, prepro_stock)\n",
    "  \n",
    "    # Compute hedging strategies on the test set\n",
    "    deltas = model_predict.predict(test_input, np.ones(batch_size)*strike, disc_batch, alpha, sess)\n",
    "        \n",
    "    # Compute CVaR 95% on the test set\n",
    "    CVaR_95_short_test   = measured_risk_exposures(deltas[:,:,0], test_input[:,:,0], disc_test, \n",
    "                            strike, position_type, prepro_stock)\n",
    "    print(\"Measured risk exposure of short position on train set: %.4f\" %(CVaR_95_short_train))\n",
    "    print(\"Measured risk exposure of short position on test set: %.4f\" %(CVaR_95_short_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Training of the long neural network and computation of the long measured risk exposure \n",
    "$$\\epsilon^{(L)}(0) = \\underset{\\delta\\in \\Pi}{\\min} \\, \\rho \\left(-\\Phi(S_{N},Z_{N}) -B_{N}G_{N}^{\\delta}\\right).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: learning rate=0.0005, layers=2, neurons=56\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'reset_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-273e285f670c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Compile the neural network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m FFNN_long = DeepAgent(train_input.shape[0], batch_size, train_input.shape[2], position_type, hidden_layers, \n\u001b[0m\u001b[0;32m      7\u001b[0m                         nbs_units, lr, prepro_stock, model_name)\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-1edb3eb44a9d>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, nbs_point_traj, batch_size, nbs_input, position_type, hidden_layers, nbs_units, lr, prepro_stock, name)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbs_point_traj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbs_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposition_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_layers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbs_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprepro_stock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnbs_point_traj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnbs_point_traj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'reset_default_graph'"
     ]
    }
   ],
   "source": [
    "position_type = 'Long'\n",
    "model_name    = \"Long_FFNN\"\n",
    "print(\"Hyperparameters: learning rate=%.4f, layers=%d, neurons=%d\" %(lr, hidden_layers, nbs_units))\n",
    "\n",
    "# Compile the neural network\n",
    "FFNN_long = DeepAgent(train_input.shape[0], batch_size, train_input.shape[2], position_type, hidden_layers, \n",
    "                        nbs_units, lr, prepro_stock, model_name)\n",
    "    \n",
    "# Start training \n",
    "start = dt.datetime.now()\n",
    "print('---Training start---')\n",
    "with tf.Session() as sess:\n",
    "    loss_train_epoch = FFNN_long.training(train_input, np.ones(batch_size)*strike, disc_batch, alpha, sess, epochs)\n",
    "   \n",
    "    print('---Training end---')\n",
    "    \n",
    "# Plot the learning curve on the train set, i.e. the CVaR on the train set at the end of each epoch\n",
    "lin_nb_epoch = np.linspace(1, loss_train_epoch.shape[0], loss_train_epoch.shape[0])\n",
    "fig = plt.figure(figsize=(5, 5), dpi=100)\n",
    "plt.plot(lin_nb_epoch[1:], loss_train_epoch[1:])\n",
    "plt.title('CVaR alpha = 0.95 of long position for ATM put option')\n",
    "plt.show()\n",
    "\n",
    "# Compute measured risk exposure on train and valid set\n",
    "model_predict = DeepAgent(test_input.shape[0], batch_size, test_input.shape[2], position_type, hidden_layers, \n",
    "                          nbs_units, lr, prepro_stock, model_name)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Load the saved model\n",
    "    model_predict.restore(sess, r\"/Users/alexa/ERP paper example/Models/%s/models.ckpt\" % model_name)\n",
    "\n",
    "    # Compute hedging strategies on the training set\n",
    "    deltas = model_predict.predict(train_input, np.ones(batch_size)*strike, disc_batch, alpha, sess)\n",
    "    \n",
    "    # Compute CVaR 95% on the train set\n",
    "    CVaR_95_long_train =  measured_risk_exposures(deltas[:,:,0], train_input[:,:,0], disc_train, strike, position_type, prepro_stock)\n",
    "  \n",
    "    # Compute hedging strategies on the test set\n",
    "    deltas = model_predict.predict(test_input, np.ones(batch_size)*strike, disc_batch, alpha, sess)\n",
    "        \n",
    "    # Compute CVaR 9% on the test set\n",
    "    CVaR_95_long_test   = measured_risk_exposures(deltas[:,:,0], test_input[:,:,0], disc_test, \n",
    "                            strike, position_type, prepro_stock)\n",
    "    print(\"Measured risk exposure of long position on train set: %.4f\" %(CVaR_95_long_train))\n",
    "    print(\"Measured risk exposure of long position on test set: %.4f\" %(CVaR_95_long_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Compute equal risk option prices $C_{0}^{\\star}$ and incompleteness measure $\\epsilon^{\\star}$ with the results of the test set:\n",
    "$$C_{0}^{\\star} = \\frac{\\epsilon^{(S)}(0) - \\epsilon^{(L)}(0)}{2B_{N}}, \\quad \\epsilon^{\\star} = \\frac{\\epsilon^{(L)}(0) + \\epsilon^{(S)}(0)}{2}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CVaR_95_short_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ff96e5c0263e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mERP\u001b[0m     \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mCVaR_95_short_test\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mCVaR_95_long_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mCVaR_95_long_test\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mCVaR_95_short_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Equal risk price of ATM put of 60 days: %.4f\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mERP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Residual hedging risk of ATM put of 60 days: %.4f\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CVaR_95_short_test' is not defined"
     ]
    }
   ],
   "source": [
    "ERP     = (CVaR_95_short_test - CVaR_95_long_test)/(2*np.exp(r*T))\n",
    "epsilon = (CVaR_95_long_test + CVaR_95_short_test)/2\n",
    "print(\"Equal risk price of ATM put of 60 days: %.4f\" %(ERP))\n",
    "print(\"Residual hedging risk of ATM put of 60 days: %.4f\" %(epsilon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "- Buehler, H., Gonon, L., Teichmann, J., and Wood, B. (2019b). Deep hedging. Quantitative Finance, 19(8):1271-1291.\n",
    "- Carbonneau, A. and Godin, F. (2020). Equal risk option pricing with deep reinforcement learning. arXiv preprint arXiv:2002.08492.\n",
    "- Godin, F. (2016). Minimizing CVaR in global dynamic hedging with transaction costs. Quantitative Finance, 16(3):461-475."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
